{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/nzxyin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[359  11  45  42  49  49  52   6  11  57  45  46  56  11  46  56  11  47\n",
      "  52  45  51  11  56  50  46  57  45   7  11  50  38  62  11  46  11  38\n",
      "  56  48  11  60  45  52  11  46  56  11  57  45  46  56  10  11 359]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from text import text_to_sequence\n",
    "from synthesize import preprocess_english\n",
    "from model import FastSpeech2\n",
    "from text.symbols import symbols\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "ckpt_path = \"/data/nzxyin/FastSPARC2/output/ckpt/LibriTTS_R_1/10000.pth.tar\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "preprocess_config = yaml.load(open(\"config/LibriTTS-R/preprocess.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "model_config = yaml.load(open(\"config/LibriTTS-R/model.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "train_config = yaml.load(open(\"config/LibriTTS-R/train.yaml\", \"r\"), Loader=yaml.FullLoader)\n",
    "\n",
    "model = FastSpeech2(model_config, preprocess_config).to(device)\n",
    "ckpt = torch.load(ckpt_path)\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "text = r\"{sil} hello, this is john smith. may I ask who is this? {sil}\"\n",
    "text = np.array(text_to_sequence(text, preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = torch.from_numpy(text).unsqueeze(0).to(device)\n",
    "text_lens = torch.tensor([texts.shape[1]]).to(device)\n",
    "max_text_len = max(text_lens)\n",
    "(\n",
    "    lr_output,\n",
    "    pitch_predictions,\n",
    "    energy_predictions,\n",
    "    log_duration_predictions,\n",
    "    duration_rounded,\n",
    "    periodicity_predictions,\n",
    "    ema_prediction,\n",
    "    src_masks,\n",
    "    bn_masks,\n",
    "    src_lens,\n",
    "    bn_lens,\n",
    ") = model(texts, text_lens, max_text_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.makedirs(\"/data/nzxyin/FastSPARC2/sample_predictions/\", exist_ok=True)\n",
    "np.save(\"/data/nzxyin/FastSPARC2/sample_predictions/pitch_predictions.npy\", pitch_predictions.detach().cpu().numpy()[0])\n",
    "np.save(\"/data/nzxyin/FastSPARC2/sample_predictions/energy_predictions.npy\", energy_predictions.detach().cpu().numpy()[0])\n",
    "np.save(\"/data/nzxyin/FastSPARC2/sample_predictions/periodicity_predictions.npy\", periodicity_predictions.detach().cpu().numpy()[0])\n",
    "np.save(\"/data/nzxyin/FastSPARC2/sample_predictions/ema_predictions.npy\", ema_prediction.detach().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0581, 0.0403, 0.0361, 0.0295, 0.0277, 0.0393, 0.0286, 0.0446, 0.0525,\n",
       "        0.0461, 0.1034, 0.0968, 0.6046, 0.5885, 0.6061, 0.5700, 0.5923, 0.6049,\n",
       "        0.5900, 0.5973, 0.5675, 0.5791, 0.5839, 0.5790, 0.7149, 0.7633, 0.7332,\n",
       "        0.7588, 0.7769, 0.7884, 0.7614, 0.7833, 0.7789, 0.7872, 0.7555, 0.7488,\n",
       "        0.7481, 0.7639, 0.7644, 0.7590, 0.7650, 0.7289, 0.6933, 0.6973, 0.6672,\n",
       "        0.6443, 0.6318, 0.6619, 0.6564, 0.6699, 0.6863, 0.6708, 0.7073, 0.7052,\n",
       "        0.6644, 0.6460, 0.5800, 0.5715, 0.5876, 0.2475, 0.2162, 0.3263, 0.3094,\n",
       "        0.3281, 0.2946, 0.2038, 0.2621, 0.2890, 0.2215, 0.2945, 0.1751, 0.2455,\n",
       "        0.4265, 0.3007, 0.3243, 0.2014, 0.1611, 0.1775, 0.2563, 0.3147, 0.3846,\n",
       "        0.3242, 0.2026, 0.2141, 0.1734, 0.3235, 0.3256, 0.3917, 0.4938, 0.5179,\n",
       "        0.5630, 0.5638, 0.5686, 0.5732, 0.5836, 0.5861, 0.5872, 0.5648, 0.5703,\n",
       "        0.5697, 0.5740, 0.5681, 0.5618, 0.5244, 0.5527, 0.5486, 0.5592, 0.4850,\n",
       "        0.4676, 0.4845, 0.4974, 0.5016, 0.4946, 0.5014, 0.4952, 0.5273, 0.5223,\n",
       "        0.5869, 0.5779, 0.5904, 0.5849, 0.5820, 0.5721, 0.5804, 0.5793, 0.5985,\n",
       "        0.5877, 0.5967, 0.6272, 0.6122, 0.6383, 0.6455, 0.6512, 0.6933, 0.6703,\n",
       "        0.6494, 0.6701, 0.6923, 0.6463, 0.7078, 0.6713, 0.6779, 0.6660, 0.6801,\n",
       "        0.6657, 0.6544, 0.6661, 0.6841, 0.6732, 0.6971, 0.6771, 0.6967, 0.6784,\n",
       "        0.6531, 0.6581, 0.6875, 0.6788, 0.7019, 0.6759, 0.6875, 0.6745, 0.7011,\n",
       "        0.6793, 0.6760, 0.6787, 0.6783, 0.6979, 0.7038, 0.6861, 0.6860, 0.6727,\n",
       "        0.7001, 0.6848, 0.6805, 0.6745, 0.6990, 0.6903, 0.6791, 0.6920, 0.6947,\n",
       "        0.6839, 0.6857, 0.6608, 0.7113, 0.6997, 0.6937, 0.6908, 0.6859, 0.7052,\n",
       "        0.7076, 0.7267, 0.6581, 0.6679, 0.6562, 0.6408, 0.6317, 0.6631, 0.6323,\n",
       "        0.6572, 0.6299, 0.6222, 0.6521, 0.6410, 0.6452, 0.6421, 0.6285, 0.6344,\n",
       "        0.6551, 0.6723, 0.6321, 0.6778, 0.6460, 0.6054, 0.6216, 0.6228, 0.6399,\n",
       "        0.6352, 0.6005, 0.6134, 0.6150, 0.6287, 0.6245, 0.6328, 0.6460, 0.6362,\n",
       "        0.6407, 0.6502, 0.6600, 0.6413, 0.6385, 0.6580, 0.6547, 0.6185, 0.6453,\n",
       "        0.6393, 0.6423, 0.6524, 0.6330, 0.6360, 0.6442, 0.6649, 0.6628, 0.6355,\n",
       "        0.6496, 0.6224, 0.6333, 0.6119, 0.6314, 0.6506, 0.6281, 0.6485, 0.6517,\n",
       "        0.6077, 0.6008, 0.6073, 0.6012, 0.5762, 0.5935, 0.5772, 0.5673, 0.5842,\n",
       "        0.5846, 0.5924, 0.5997, 0.6014, 0.5815, 0.5758, 0.5687, 0.5787, 0.5745,\n",
       "        0.5962, 0.5978, 0.6043, 0.5859, 0.6048, 0.6205, 0.5812, 0.5772, 0.5796,\n",
       "        0.5879, 0.5667, 0.5746, 0.5934, 0.5897, 0.5831, 0.5932, 0.5908, 0.5723,\n",
       "        0.5940, 0.5609, 0.5763, 0.5744, 0.5572, 0.5615, 0.5460, 0.5420, 0.5448,\n",
       "        0.5441, 0.5437, 0.5611, 0.5492, 0.5334, 0.5356, 0.5366, 0.5362, 0.5339,\n",
       "        0.5406, 0.5591, 0.5968, 0.5758, 0.5912, 0.6215, 0.6077, 0.6203, 0.5808,\n",
       "        0.6081, 0.6213, 0.6358, 0.6422, 0.6331, 0.6235, 0.5976, 0.6057, 0.6294,\n",
       "        0.6135, 0.6366, 0.6140, 0.6155, 0.5982, 0.5531, 0.5183, 0.4709, 0.4765,\n",
       "        0.4888, 0.4641, 0.4765, 0.4725, 0.4627, 0.4694, 0.4601, 0.4866, 0.4685,\n",
       "        0.4664, 0.4808, 0.4950, 0.5086, 0.5023, 0.5257, 0.5000, 0.5220, 0.5173,\n",
       "        0.5266, 0.5250, 0.5362, 0.5169, 0.5244, 0.5218, 0.5279, 0.5193, 0.5537,\n",
       "        0.5558, 0.5600, 0.5658, 0.5925, 0.5782, 0.5915, 0.5987, 0.6042, 0.5697,\n",
       "        0.5800, 0.5766, 0.5686, 0.5736, 0.5700, 0.5838, 0.5749, 0.5948, 0.5953,\n",
       "        0.5600, 0.5633, 0.5840, 0.5708, 0.5791, 0.5848, 0.5779, 0.5568, 0.5878,\n",
       "        0.6003, 0.6055, 0.5638, 0.5593, 0.5695, 0.5786, 0.5749, 0.5835, 0.5798,\n",
       "        0.5845, 0.5865, 0.5664, 0.5608, 0.5772, 0.5408, 0.5555, 0.5715, 0.5774,\n",
       "        0.5692, 0.5774, 0.5778, 0.5823, 0.5810, 0.5819, 0.5751, 0.5560, 0.5635,\n",
       "        0.5613, 0.5569, 0.5577, 0.5494, 0.5295, 0.5530, 0.5458, 0.5456, 0.5566,\n",
       "        0.5616, 0.5611, 0.5582, 0.5847, 0.5566, 0.5727, 0.5657, 0.5714, 0.5530,\n",
       "        0.5574, 0.5649, 0.5662, 0.5663, 0.5685, 0.5589, 0.5909, 0.5758, 0.5727,\n",
       "        0.5801, 0.5574, 0.5647, 0.5623, 0.5790, 0.5720, 0.5531, 0.5790, 0.5790,\n",
       "        0.5574, 0.5737, 0.5211, 0.5441, 0.5362, 0.5240, 0.5169, 0.5210, 0.5256,\n",
       "        0.5234, 0.5205, 0.5094, 0.5198, 0.5032, 0.5179, 0.5097, 0.5069, 0.5144,\n",
       "        0.4946, 0.5018, 0.5120, 0.5131, 0.5232, 0.5091, 0.5083, 0.5067, 0.4978,\n",
       "        0.4951, 0.5078, 0.5090, 0.5055, 0.4936, 0.5098, 0.5185, 0.5278, 0.5226,\n",
       "        0.5109, 0.5074, 0.5308, 0.5176, 0.5199, 0.5310, 0.5203, 0.5428, 0.5253,\n",
       "        0.5293, 0.5177, 0.5164, 0.5017, 0.4875, 0.4863, 0.4849, 0.4943, 0.4936,\n",
       "        0.5045, 0.4938, 0.4941, 0.4864, 0.4846, 0.4851, 0.4873, 0.4861, 0.4877,\n",
       "        0.4736, 0.4796, 0.4862, 0.4903, 0.5067, 0.4971, 0.4991, 0.4869, 0.4873,\n",
       "        0.5034, 0.4962, 0.4974, 0.5095, 0.4816, 0.4827, 0.4922, 0.4861, 0.4902,\n",
       "        0.4867, 0.4868, 0.4716, 0.4880, 0.4808, 0.4829, 0.4923, 0.4898, 0.4847,\n",
       "        0.4799, 0.4797, 0.4858, 0.4737, 0.4706, 0.4733, 0.4862, 0.4748, 0.4773,\n",
       "        0.4804, 0.4738, 0.4812, 0.4744, 0.4740, 0.4900, 0.4921, 0.4901, 0.4849,\n",
       "        0.4933, 0.4902, 0.4840, 0.4834, 0.4809, 0.4791, 0.4885, 0.4745, 0.4810,\n",
       "        0.4827, 0.5007, 0.4945, 0.4915, 0.4886, 0.4980, 0.5011, 0.4992, 0.4941,\n",
       "        0.4786, 0.4822, 0.4948, 0.4846, 0.4793, 0.4809, 0.4793, 0.4791, 0.4774,\n",
       "        0.4824, 0.4754, 0.4894, 0.4772, 0.4833, 0.4884, 0.4768, 0.4839, 0.4677,\n",
       "        0.4606, 0.4803, 0.4830, 0.5026, 0.4965, 0.5192, 0.5056, 0.5038, 0.4917,\n",
       "        0.4987, 0.4902, 0.4867, 0.4823, 0.4814, 0.4818, 0.4906, 0.4864, 0.4939,\n",
       "        0.4985, 0.5046, 0.4975, 0.4934, 0.4913, 0.4945, 0.4823, 0.4724, 0.4638,\n",
       "        0.4581, 0.4608, 0.4542, 0.4542, 0.4522, 0.4487, 0.4455, 0.4536, 0.4569,\n",
       "        0.4630, 0.4613, 0.4576, 0.4520, 0.4590, 0.4662, 0.4698, 0.4840, 0.4737,\n",
       "        0.4934, 0.4856, 0.4827, 0.4877, 0.4811, 0.4813, 0.4753, 0.4784, 0.4881,\n",
       "        0.4846, 0.4836, 0.4905, 0.4801, 0.4859, 0.4851, 0.4879, 0.4806, 0.4781,\n",
       "        0.4784, 0.4698, 0.4722, 0.4765, 0.4801, 0.4763, 0.4700, 0.4787, 0.4838,\n",
       "        0.4735, 0.4719, 0.4714, 0.4729, 0.4739, 0.4736, 0.4738, 0.4751, 0.4750,\n",
       "        0.4748, 0.4798, 0.4687, 0.4820, 0.4848, 0.4777, 0.4687, 0.4728, 0.4717,\n",
       "        0.4766, 0.4777, 0.4781, 0.4718, 0.4645, 0.4586, 0.4456, 0.4362, 0.4412,\n",
       "        0.4389, 0.4472, 0.4333, 0.4434, 0.4429, 0.4447, 0.4419, 0.4443, 0.4384,\n",
       "        0.4424, 0.4370, 0.4384, 0.4334, 0.4444, 0.4337, 0.4341, 0.4363, 0.4288,\n",
       "        0.4288, 0.4264, 0.4375, 0.4414, 0.4349, 0.4380, 0.4387, 0.4270, 0.4346,\n",
       "        0.4301, 0.4424, 0.4377, 0.4388, 0.4354, 0.4401, 0.4436, 0.4427, 0.4429,\n",
       "        0.4388, 0.4452, 0.4373, 0.4507, 0.4444, 0.4403, 0.4391, 0.4441, 0.4452,\n",
       "        0.4429, 0.4427, 0.4430, 0.4466, 0.4363, 0.4409, 0.4513, 0.4490, 0.4623,\n",
       "        0.4601, 0.4597, 0.4588, 0.4500, 0.4622, 0.4584, 0.4662, 0.4701, 0.4588,\n",
       "        0.4525, 0.4461, 0.4511, 0.4504, 0.4604, 0.4569, 0.4569, 0.4604, 0.4409,\n",
       "        0.4629, 0.4536, 0.4518, 0.4473, 0.4509, 0.4646, 0.4483, 0.4556, 0.4507,\n",
       "        0.4666, 0.4621, 0.4594, 0.4735, 0.4800, 0.4919, 0.4835, 0.5032, 0.4934,\n",
       "        0.4892, 0.4970, 0.4807, 0.4968, 0.4782, 0.4810, 0.4960, 0.4984, 0.4946,\n",
       "        0.4902, 0.4813, 0.4900, 0.4826, 0.4809, 0.4962, 0.4905, 0.4781, 0.4885,\n",
       "        0.4909, 0.4835, 0.4810, 0.4791, 0.4963, 0.4832, 0.4901, 0.4784, 0.5011,\n",
       "        0.4929, 0.4996, 0.4813, 0.4673, 0.4732, 0.4732, 0.4682, 0.4701, 0.4769,\n",
       "        0.4719, 0.4689, 0.4632, 0.4774, 0.4655, 0.4753, 0.4652, 0.4771, 0.4756,\n",
       "        0.4882, 0.4872, 0.4872, 0.4853, 0.4863, 0.4894, 0.4812, 0.4811, 0.4753,\n",
       "        0.4776, 0.4938, 0.4843, 0.4990, 0.5052, 0.4998, 0.4915, 0.4804, 0.4852,\n",
       "        0.4885, 0.4891, 0.4987, 0.4915, 0.4819, 0.4923, 0.4923, 0.4817, 0.4761,\n",
       "        0.4763, 0.4719, 0.4799, 0.4824, 0.4946, 0.4909, 0.4948, 0.4845, 0.4974,\n",
       "        0.4794, 0.4803, 0.4818, 0.4930, 0.4889, 0.4924, 0.5009, 0.4860, 0.4818,\n",
       "        0.4857, 0.4751, 0.4866, 0.4905, 0.4950, 0.5090, 0.4985, 0.4891, 0.4858,\n",
       "        0.4798, 0.4802, 0.5024, 0.5088, 0.5067, 0.4988, 0.4814, 0.4972, 0.4941,\n",
       "        0.4865, 0.4870, 0.4851, 0.4903, 0.5018, 0.5093, 0.4932, 0.4910, 0.5003,\n",
       "        0.5145, 0.4958, 0.4950, 0.5016, 0.5085, 0.4903, 0.4969, 0.5012, 0.5122,\n",
       "        0.5226, 0.5251, 0.5146, 0.5075, 0.4979, 0.5072, 0.4995, 0.5084, 0.5089,\n",
       "        0.4929, 0.4840, 0.4951, 0.4992, 0.5147, 0.4985, 0.5001, 0.4948, 0.5020,\n",
       "        0.4912, 0.4994, 0.5074, 0.5159, 0.5182, 0.5080, 0.5078, 0.4898, 0.4992,\n",
       "        0.5046, 0.4901, 0.4987, 0.4959, 0.4943, 0.5076, 0.5098, 0.4948, 0.4784,\n",
       "        0.4830, 0.4821, 0.4901, 0.4755, 0.4777, 0.4829, 0.4692, 0.4838, 0.4743,\n",
       "        0.4799, 0.4938, 0.5171], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periodicity_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11., 17., 13., 14., 16., 16., 12., 12., 21., 17., 16., 17., 18., 19.,\n",
       "        21., 20., 25., 19., 16., 17., 15., 23., 20., 21., 16., 17., 23., 22.,\n",
       "        19., 18., 16., 20., 18., 16., 21., 16., 16., 17., 21., 18., 17., 15.,\n",
       "        16., 16., 16., 17., 15., 16., 17., 19., 15., 24., 10.],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_rounded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastsparc2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
